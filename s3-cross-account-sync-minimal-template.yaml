AWSTemplateFormatVersion: '2010-09-09'
Description: 'S3 Cross-Partition Sync Solution - Simplified Single Lambda'

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: "Required Configuration"
        Parameters:
          - SourceRegion
          - SourceBucketName
          - TargetRegion
          - TargetBucketName
          - TargetAccessKeyId
          - TargetSecretAccessKey
          - MaxFileSizeCategory
      - Label:
          default: "Optional Settings"
        Parameters:
          - SecretsManagerSecretName
          - S3PrefixFilter
          - RetryAttempts
          - EnableSNSNotification
          - NotificationEmail
    ParameterLabels:
      SourceRegion:
        default: "Source Region (AWS China)"
      SourceBucketName:
        default: "Source S3 Bucket Name"
      TargetRegion:
        default: "Target Region (AWS Global)"
      TargetBucketName:
        default: "Target S3 Bucket Name"
      TargetAccessKeyId:
        default: "Target Account Access Key ID"
      TargetSecretAccessKey:
        default: "Target Account Secret Access Key"
      SecretsManagerSecretName:
        default: "Secrets Manager Secret Name"
      MaxFileSizeCategory:
        default: "Maximum File Size Category"
      S3PrefixFilter:
        default: "S3 Prefix Filter (Optional)"
      RetryAttempts:
        default: "Retry Attempts"
      EnableSNSNotification:
        default: "Enable SNS Email Notification"
      NotificationEmail:
        default: "Notification Email Address"

Parameters:
  SourceRegion:
    Type: String
    Default: 'cn-northwest-1'
    Description: 'Source region in AWS China'
    AllowedValues: 
      - 'cn-northwest-1'
      - 'cn-north-1'

  SourceBucketName:
    Type: String
    Description: 'Source S3 bucket name (in AWS China region)'
    MinLength: 3
    MaxLength: 63
    AllowedPattern: '^[a-z0-9][a-z0-9-]*[a-z0-9]$'

  TargetRegion:
    Type: String
    Default: 'us-west-2'
    Description: 'Target region (AWS Global)'
    AllowedValues: 
      # US Regions
      - 'us-east-1'
      - 'us-east-2'
      - 'us-west-1'
      - 'us-west-2'
      # Europe Regions
      - 'eu-west-1'
      - 'eu-west-2'
      - 'eu-west-3'
      - 'eu-central-1'
      - 'eu-north-1'
      - 'eu-south-1'
      # Asia Pacific Regions
      - 'ap-southeast-1'
      - 'ap-southeast-2'
      - 'ap-northeast-1'
      - 'ap-northeast-2'
      - 'ap-northeast-3'
      - 'ap-south-1'
      - 'ap-east-1'
      # Other Regions
      - 'ca-central-1'
      - 'sa-east-1'
      - 'af-south-1'
      - 'me-south-1'

  TargetBucketName:
    Type: String
    Description: 'Target S3 bucket name (in AWS Global region)'
    MinLength: 3
    MaxLength: 63
    AllowedPattern: '^[a-z0-9][a-z0-9-]*[a-z0-9]$'

  TargetAccessKeyId:
    Type: String
    Description: 'AWS Access Key ID for target account'
    MinLength: 16
    MaxLength: 128
    NoEcho: true

  TargetSecretAccessKey:
    Type: String
    Description: 'AWS Secret Access Key for target account'
    MinLength: 1
    NoEcho: true

  SecretsManagerSecretName:
    Type: String
    Default: 's3-cross-partition-sync-credentials'
    Description: 'Name for the Secrets Manager secret'
    MinLength: 1
    MaxLength: 512

  MaxFileSizeCategory:
    Type: String
    Description: 'Maximum expected file size category - determines Lambda memory and /tmp directory allocation. Files larger than 10GB are not supported.'
    AllowedValues:
      - 'Under200MB'
      - '200MB-2GB'
      - '2GB-10GB'
    ConstraintDescription: 'Must be Under200MB, 200MB-2GB, or 2GB-10GB. Files larger than 10GB are not supported.'

  S3PrefixFilter:
    Type: String
    Default: ''
    Description: 'Optional S3 prefix filter (e.g., "documents/", "images/2024/"). Leave empty to sync all objects.'
    MaxLength: 1024

  RetryAttempts:
    Type: Number
    Default: 3
    MinValue: 2
    MaxValue: 10
    Description: 'Number of retry attempts for failed transfers (2-10)'
    ConstraintDescription: 'Must be between 2 and 10'

  EnableSNSNotification:
    Type: String
    Default: 'false'
    AllowedValues: ['true', 'false']
    Description: 'Enable SNS email notifications for sync failures'

  NotificationEmail:
    Type: String
    Default: ''
    Description: 'Email address for failure notifications (required if SNS is enabled)'
    AllowedPattern: '^$|^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    ConstraintDescription: 'Must be a valid email address or empty'

Mappings:
  FileSizeConfig:
    Under200MB:
      MemorySize: 256
      TmpDirectorySize: 512
      ProcessingMode: 'memory'
      ThresholdMB: 200
      TimeoutSeconds: 180
    200MB-2GB:
      MemorySize: 512
      TmpDirectorySize: 3072
      ProcessingMode: 'tmp'
      ThresholdMB: 2048
      TimeoutSeconds: 600
    2GB-10GB:
      MemorySize: 1024
      TmpDirectorySize: 10240
      ProcessingMode: 'tmp'
      ThresholdMB: 10240
      TimeoutSeconds: 900

Conditions:
  CreateSNSNotification: !Equals [!Ref EnableSNSNotification, 'true']

Resources:
  # Secrets Manager Secret
  TargetAccountCredentials:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Ref SecretsManagerSecretName
      Description: 'Cross-partition S3 sync target account credentials'
      SecretString: !Sub |
        {
          "AccessKeyId": "${TargetAccessKeyId}",
          "SecretAccessKey": "${TargetSecretAccessKey}"
        }

  # Lambda execution role
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - !Sub 'arn:aws-cn:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
      Policies:
        - PolicyName: S3CrossPartitionSyncPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action: ['s3:GetObject', 's3:HeadObject', 's3:ListBucket']
                Resource: 
                  - !Sub 'arn:aws-cn:s3:::${SourceBucketName}'
                  - !Sub 'arn:aws-cn:s3:::${SourceBucketName}/*'
              - Effect: Allow
                Action: 'secretsmanager:GetSecretValue'
                Resource: !Ref TargetAccountCredentials
              - Effect: Allow
                Action: ['sns:Publish']
                Resource: !If [CreateSNSNotification, !Ref FailureNotificationTopic, !Ref 'AWS::NoValue']

  # SNS Topic for failure notifications
  FailureNotificationTopic:
    Type: AWS::SNS::Topic
    Condition: CreateSNSNotification
    Properties:
      TopicName: 'S3CrossPartitionSyncFailures'
      DisplayName: 'S3 Cross-Partition Sync Failure Notifications'

  # SNS Subscription for email notifications
  FailureNotificationSubscription:
    Type: AWS::SNS::Subscription
    Condition: CreateSNSNotification
    Properties:
      TopicArn: !Ref FailureNotificationTopic
      Protocol: email
      Endpoint: !Ref NotificationEmail

  # Main Lambda function for S3 sync
  SyncLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: 'S3CrossPartitionSync'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Timeout: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, TimeoutSeconds]
      MemorySize: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, MemorySize]
      EphemeralStorage:
        Size: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, TmpDirectorySize]
      Environment:
        Variables:
          SOURCE_REGION: !Ref SourceRegion
          TARGET_REGION: !Ref TargetRegion
          SOURCE_BUCKET: !Ref SourceBucketName
          TARGET_BUCKET: !Ref TargetBucketName
          SECRETS_MANAGER_SECRET_NAME: !Ref SecretsManagerSecretName
          S3_PREFIX_FILTER: !Ref S3PrefixFilter
          MAX_FILE_SIZE_CATEGORY: !Ref MaxFileSizeCategory
          FILE_SIZE_THRESHOLD_MB: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, ThresholdMB]
          PROCESSING_MODE: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, ProcessingMode]
          RETRY_ATTEMPTS: !Ref RetryAttempts
          ENABLE_SNS_NOTIFICATION: !Ref EnableSNSNotification
          SNS_TOPIC_ARN: !If [CreateSNSNotification, !Ref FailureNotificationTopic, '']
      Code:
        ZipFile: |
          import json
          import boto3
          import logging
          from botocore.exceptions import ClientError
          import os
          import urllib.parse
          from datetime import datetime
          import time

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          class CrossPartitionS3Sync:
              def __init__(self):
                  # Environment variables
                  self.source_region = os.environ['SOURCE_REGION']
                  self.target_region = os.environ['TARGET_REGION']
                  self.source_bucket = os.environ['SOURCE_BUCKET']
                  self.target_bucket = os.environ['TARGET_BUCKET']
                  self.secrets_name = os.environ['SECRETS_MANAGER_SECRET_NAME']
                  self.prefix_filter = os.environ.get('S3_PREFIX_FILTER', '').strip()
                  self.file_size_threshold_mb = int(os.environ.get('FILE_SIZE_THRESHOLD_MB', '200'))
                  self.processing_mode = os.environ.get('PROCESSING_MODE', 'memory')
                  self.retry_attempts = int(os.environ.get('RETRY_ATTEMPTS', '3'))
                  self.enable_sns = os.environ.get('ENABLE_SNS_NOTIFICATION', 'false').lower() == 'true'
                  self.sns_topic_arn = os.environ.get('SNS_TOPIC_ARN', '')
                  
                  # Initialize clients
                  self.source_s3 = boto3.client('s3', region_name=self.source_region)
                  self.secrets_client = boto3.client('secretsmanager', region_name=self.source_region)
                  if self.enable_sns and self.sns_topic_arn:
                      self.sns_client = boto3.client('sns', region_name=self.source_region)
                  
                  # Get target credentials and initialize target S3 client
                  credentials = self._get_target_credentials()
                  self.target_s3 = boto3.client(
                      's3',
                      region_name=self.target_region,
                      aws_access_key_id=credentials['AccessKeyId'],
                      aws_secret_access_key=credentials['SecretAccessKey']
                  )

              def _get_target_credentials(self):
                  """Get target credentials from Secrets Manager"""
                  try:
                      response = self.secrets_client.get_secret_value(SecretId=self.secrets_name)
                      return json.loads(response['SecretString'])
                  except Exception as e:
                      logger.error(f"Failed to get credentials: {e}")
                      raise

              def get_object_metadata(self, s3_client, bucket, key):
                  """Get S3 object metadata"""
                  try:
                      response = s3_client.head_object(Bucket=bucket, Key=key)
                      return {
                          'etag': response.get('ETag', '').strip('"'),
                          'size': response.get('ContentLength', 0),
                          'content_type': response.get('ContentType')
                      }
                  except ClientError as e:
                      if e.response['Error']['Code'] == '404':
                          return None
                      raise

              def should_sync_object(self, key):
                  """Check if object key matches the prefix filter"""
                  return not self.prefix_filter or key.startswith(self.prefix_filter)

              def send_failure_notification(self, key, error_message, attempts):
                  """Send SNS notification for sync failure"""
                  if not self.enable_sns or not self.sns_topic_arn:
                      return
                  
                  try:
                      subject = f"S3 Cross-Partition Sync Failure: {key}"
                      message = ("S3 Cross-Partition Sync Failure Report\\n\\n"
                                f"Object: {key}\\n"
                                f"Source Bucket: {self.source_bucket} ({self.source_region})\\n"
                                f"Target Bucket: {self.target_bucket} ({self.target_region})\\n"
                                f"Attempts Made: {attempts}/{self.retry_attempts}\\n"
                                f"Error: {error_message}\\n"
                                f"Timestamp: {datetime.utcnow().isoformat()}Z\\n\\n"
                                "Please check the CloudWatch logs for more details.")
                      
                      self.sns_client.publish(TopicArn=self.sns_topic_arn, Subject=subject, Message=message)
                      logger.info(f"Failure notification sent for {key}")
                  except Exception as e:
                      logger.error(f"Failed to send SNS notification: {e}")

              def copy_object(self, key):
                  """Copy object from source to target with retry mechanism"""
                  # Check prefix filter first
                  if not self.should_sync_object(key):
                      logger.info(f"Skipping {key} - does not match prefix filter: {self.prefix_filter}")
                      return True
                  
                  for attempt in range(1, self.retry_attempts + 1):
                      try:
                          logger.info(f"Attempt {attempt}/{self.retry_attempts} for {key}")
                          
                          # Check if sync needed
                          source_meta = self.get_object_metadata(self.source_s3, self.source_bucket, key)
                          if not source_meta:
                              logger.error(f"Source object {key} not found")
                              return False
                          
                          target_meta = self.get_object_metadata(self.target_s3, self.target_bucket, key)
                          if target_meta and (source_meta['etag'] == target_meta['etag'] and source_meta['size'] == target_meta['size']):
                              logger.info(f"Object {key} already in sync")
                              return True
                          
                          file_size_mb = source_meta['size'] / (1024 * 1024)
                          logger.info(f"Copying {key}, size: {file_size_mb:.2f}MB from {self.source_region} to {self.target_region}")
                          
                          # Check if file size exceeds configured threshold
                          if file_size_mb > self.file_size_threshold_mb:
                              logger.warning(f"File {key} ({file_size_mb:.2f}MB) exceeds configured threshold ({self.file_size_threshold_mb}MB)")
                              return False
                          
                          # Copy file based on processing mode and size
                          success = self._copy_file(key, source_meta, file_size_mb)
                          
                          if success:
                              logger.info(f"Successfully copied {key} on attempt {attempt}")
                              return True
                          else:
                              logger.warning(f"Failed to copy {key} on attempt {attempt}")
                              
                      except Exception as e:
                          logger.error(f"Attempt {attempt}/{self.retry_attempts} failed for {key}: {e}")
                          if attempt == self.retry_attempts:
                              logger.error(f"All {self.retry_attempts} attempts failed for {key}")
                              self.send_failure_notification(key, str(e), attempt)
                              return False
                          else:
                              wait_time = min(2 ** (attempt - 1), 30)  # Exponential backoff, max 30s
                              logger.info(f"Waiting {wait_time}s before retry...")
                              time.sleep(wait_time)
                  
                  return False

              def _copy_file(self, key, source_meta, file_size_mb):
                  """Copy file using appropriate method based on size and processing mode"""
                  if self.processing_mode == 'memory' or file_size_mb <= 200:
                      return self._copy_via_memory(key, source_meta)
                  else:
                      return self._copy_via_tmp(key, source_meta)

              def _copy_via_memory(self, key, source_meta):
                  """Copy file using memory"""
                  source_obj = self.source_s3.get_object(Bucket=self.source_bucket, Key=key)
                  put_args = {'Bucket': self.target_bucket, 'Key': key, 'Body': source_obj['Body'].read()}
                  if source_meta['content_type']:
                      put_args['ContentType'] = source_meta['content_type']
                  
                  self.target_s3.put_object(**put_args)
                  logger.info(f"Successfully copied file {key} via memory")
                  return True

              def _copy_via_tmp(self, key, source_meta):
                  """Copy file using /tmp directory"""
                  temp_file_path = f"/tmp/{os.path.basename(key)}"
                  
                  try:
                      # Download to /tmp
                      logger.info(f"Downloading file {key} to /tmp")
                      self.source_s3.download_file(self.source_bucket, key, temp_file_path)
                      
                      # Upload from /tmp
                      logger.info(f"Uploading file {key} from /tmp")
                      extra_args = {}
                      if source_meta['content_type']:
                          extra_args['ContentType'] = source_meta['content_type']
                      
                      self.target_s3.upload_file(temp_file_path, self.target_bucket, key, ExtraArgs=extra_args)
                      logger.info(f"Successfully copied file {key} via /tmp")
                      return True
                      
                  finally:
                      # Clean up temp file
                      if os.path.exists(temp_file_path):
                          os.remove(temp_file_path)
                          logger.info(f"Cleaned up temp file: {temp_file_path}")

              def process_event(self, records):
                  """Process S3 event records"""
                  results = {'success': [], 'failed': []}
                  
                  for record in records:
                      try:
                          if record.get('eventSource') == 'aws:s3':
                              key = urllib.parse.unquote_plus(record['s3']['object']['key'])
                              (results['success'] if self.copy_object(key) else results['failed']).append(key)
                      except Exception as e:
                          logger.error(f"Error processing record: {e}")
                          results['failed'].append(str(record))
                  
                  return results

          def lambda_handler(event, context):
              try:
                  sync = CrossPartitionS3Sync()
                  
                  # Handle different event types
                  if 'Records' in event:
                      # Direct S3 event format
                      results = sync.process_event(event['Records'])
                  elif 'detail' in event and 'object' in event['detail']:
                      # EventBridge format from S3
                      key = event['detail']['object']['key']
                      success = sync.copy_object(key)
                      results = {'success': [key] if success else [], 'failed': [] if success else [key]}
                  elif 'test_key' in event:
                      # Manual test
                      key = event['test_key']
                      success = sync.copy_object(key)
                      results = {'success': [key] if success else [], 'failed': [] if success else [key]}
                  else:
                      raise ValueError("Unsupported event format")
                  
                  return {
                      'statusCode': 200,
                      'body': {
                          'message': 'Cross-partition S3 sync completed',
                          'results': results
                      }
                  }
                  
              except Exception as e:
                  logger.error(f"Lambda execution failed: {e}")
                  return {'statusCode': 500, 'body': {'error': str(e)}}

  # EventBridge rule to capture S3 events
  S3EventRule:
    Type: AWS::Events::Rule
    Properties:
      Description: 'Trigger sync on S3 object creation'
      EventPattern:
        source: ['aws.s3']
        detail-type: ['Object Created']
        detail:
          bucket:
            name: [!Ref SourceBucketName]
      State: ENABLED
      Targets:
        - Arn: !GetAtt SyncLambdaFunction.Arn
          Id: S3SyncTarget

  # Lambda permission for EventBridge
  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref SyncLambdaFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt S3EventRule.Arn

Outputs:
  LambdaFunctionName:
    Description: 'Sync Lambda Function Name'
    Value: !Ref SyncLambdaFunction

  SecretsManagerSecretName:
    Description: 'Secrets Manager Secret Name'
    Value: !Ref SecretsManagerSecretName

  EventBridgeRuleName:
    Description: 'EventBridge Rule Name'
    Value: !Ref S3EventRule

  SetupInfo:
    Description: 'Setup Information and Next Steps'
    Value: !Sub |
      ✅ S3 Cross-partition sync deployed successfully!
      
      📍 Configuration:
      • Source: ${SourceBucketName} (${SourceRegion})
      • Target: ${TargetBucketName} (${TargetRegion})
      • Lambda: ${SyncLambdaFunction}
      • EventBridge Rule: ${S3EventRule}
      • Prefix Filter: ${S3PrefixFilter}
      
      🔒 Credentials: Stored in Secrets Manager (${SecretsManagerSecretName})
      📊 File Size Category: ${MaxFileSizeCategory}
      🔄 Retry Attempts: ${RetryAttempts}
      📧 SNS Notifications: ${EnableSNSNotification}
      
      ⚠️  IMPORTANT - Manual Setup Required:
      1. Enable EventBridge notifications on source S3 bucket:
         aws s3api put-bucket-notification-configuration \
           --bucket ${SourceBucketName} \
           --notification-configuration '{"EventBridgeConfiguration": {}}'
      
      2. Create target bucket in target account:
         aws s3 mb s3://${TargetBucketName} --region ${TargetRegion}
      
      3. Ensure target Access Key has S3 permissions for ${TargetBucketName}

  TestCommand:
    Description: 'Test sync function manually'
    Value: !Sub |
      aws lambda invoke --function-name ${SyncLambdaFunction} --payload '{"test_key":"test/sample.txt"}' response.json

  S3EventBridgeSetupCommand:
    Description: 'Command to enable S3 EventBridge notifications'
    Value: !Sub |
      aws s3api put-bucket-notification-configuration --bucket ${SourceBucketName} --notification-configuration '{"EventBridgeConfiguration": {}}'

  LambdaConfiguration:
    Description: 'Lambda Configuration Details'
    Value: !Sub 
      - |
        📊 File Size Category: ${MaxFileSizeCategory}
        💾 Lambda Memory: ${MemorySize}MB  
        💽 Lambda /tmp Directory: ${TmpSize}MB
        📏 Max File Size Threshold: ${ThresholdMB}MB
        ⏱️ Timeout: ${TimeoutSeconds}s
      - MemorySize: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, MemorySize]
        TmpSize: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, TmpDirectorySize]
        ThresholdMB: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, ThresholdMB]
        TimeoutSeconds: !FindInMap [FileSizeConfig, !Ref MaxFileSizeCategory, TimeoutSeconds]

  SNSTopicArn:
    Description: 'SNS Topic ARN for failure notifications'
    Condition: CreateSNSNotification
    Value: !Ref FailureNotificationTopic

  SNSSetupInfo:
    Description: 'SNS Notification Setup Information'
    Condition: CreateSNSNotification
    Value: !Sub |
      📧 SNS Topic: ${FailureNotificationTopic}
      📬 Email: ${NotificationEmail}
      ⚠️  Please check your email and confirm the SNS subscription! 